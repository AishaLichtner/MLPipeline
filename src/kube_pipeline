import kfp
from kfp.v2 import compiler
from kfp.v2.dsl import component, Output, Model, Input
import requests
import pickle
import os
from google.cloud import aiplatform
from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp
from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp

project_id = "celtic-sunlight-424420-b8"
pipeline_root_path = "https://console.cloud.google.com/storage/browser/aisha-bucket"
github_url = "https://github.com/AishaLichtner/MLPipeline/blob/main/outputs/models/1036708_model.pkl"

@component
def download_and_load_model_component(github_url: str, model_output: Output[Model]) -> None:
    import requests
    import pickle
    import os
    def download_and_load_model(github_url: str, local_path: str = '/tmp/model.pkl', ):
        """
        Download a model from GitHub and load it from a pickle file.
        
        Args:
            github_url (str): The GitHub URL to the pickle file containing the model.
            local_path (str): The local path to save the downloaded pickle file.
        
        Returns:
            model: The loaded model.
        """
        response = requests.get(github_url)
        response.raise_for_status()
        
        with open(local_path, 'wb') as file:
            file.write(response.content)
        
        with open(local_path, 'rb') as file:
            model = pickle.load(file)
        return model
    download_and_load_model(github_url)

@component
def deploy_model_custom_trained_model_sample(
    project: str,
    location: str,
    model: Input[Model],
    deployed_model_display_name: str,
    api_endpoint: str = "us-central1-aiplatform.googleapis.com",
    timeout: int = 7200,
) -> str:
    from google.cloud import aiplatform
    from google.cloud.aiplatform.gapic import EndpointServiceClient

    client_options = {"api_endpoint": api_endpoint}
    client = EndpointServiceClient(client_options=client_options)

    endpoint = client.create_endpoint(
        parent=f"projects/{project}/locations/{location}",
        endpoint={"display_name": deployed_model_display_name}
    )
    endpoint_id = endpoint.result().name

    deployed_model = {
        "model": model.uri,
        "display_name": deployed_model_display_name,
        "dedicated_resources": {
            "min_replica_count": 1,
            "machine_spec": {
                "machine_type": "n1-standard-2",
            },
        },
    }
    traffic_split = {"0": 100}
    response = client.deploy_model(
        endpoint=endpoint_id, deployed_model=deployed_model, traffic_split=traffic_split
    )
    response.result(timeout=timeout)
    return endpoint_id


# Workflow of the pipeline
# low of the pipeline.
@kfp.dsl.pipeline(
    name="forecast_deployment_pipeline",
    pipeline_root=pipeline_root_path)
def pipeline(project_id: str):
    # The first step of your workflow is a dataset generator.
    # This step takes a Google Cloud Pipeline Component, providing the necessary
    # input arguments, and uses the Python variable `ds_op` to define its
    # output. Note that here the `ds_op` only stores the definition of the
    # output but not the actual returned object from the execution. The value
    # of the object is not accessible at the dsl.pipeline level, and can only be
    # retrieved by providing it as the input to a downstream component.

    ds_op = TabularDatasetCreateOp(
        display_name = "future", 
        gcs_source= "gs://aisha-bucket",
        project = project_id)
    
    # The second step is loading the trained model from GitHub
    download_and_load_model_op = download_and_load_model_component(github_url=github_url)

    #third step is create and endpoint and deploy the model to an endpoint
    deploy_model_op = deploy_model_custom_trained_model_sample(
        project=project_id,
        location="us-central1",
        model=download_and_load_model_op.outputs['model_output'],
        deployed_model_display_name="my-prophet-model"
    )

    if __name__ == "__main__":
        client = kfp.Client()
        compiler.Compiler().compile(
        pipeline_func=pipeline,
        package_path='forcast_deployment_pipeline.yaml'
)